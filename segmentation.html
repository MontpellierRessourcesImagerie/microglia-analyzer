

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Segmentation using a UNet2D &mdash; Microglia Analyzer 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=5b94cef0" />

  
    <link rel="shortcut icon" href="_static/icon.png"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=d45e8c67"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Classification using a YOLOv5" href="classification.html" />
    <link rel="prev" title="How to use the “Microglia Analyzer” widget?" href="mga_user_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Microglia Analyzer
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="user_guide.html">Quick start: A user guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Segmentation using a UNet2D</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-unet2d">0. What is UNet2D?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#get-your-data-ready">1. Get your data ready</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-augmentation">2. Data augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#filaments-extraction">3. Filaments extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup">4. Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage">5. Usage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">Classification using a YOLOv5</a></li>
<li class="toctree-l1"><a class="reference internal" href="measures.html">Measures on microglia</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Microglia Analyzer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Segmentation using a UNet2D</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/segmentation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="segmentation-using-a-unet2d">
<h1>Segmentation using a UNet2D<a class="headerlink" href="#segmentation-using-a-unet2d" title="Link to this heading"></a></h1>
<section id="what-is-unet2d">
<h2>0. What is UNet2D?<a class="headerlink" href="#what-is-unet2d" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>UNet2D is a deep-learning architecture in the family of convolutional neural-networks and in the sub-family of auto-encoders.</p></li>
<li><p>It is trained through supervized learning, which means that for training, some pairs of input image + expected segmentation (== ground-truth) are required.</p></li>
<li><p>After training, the model is able to produce a probability map through its process of inference. This probability map has to be thresholded to transform it into a mask.</p></li>
<li><p>UNet2D generates a semantic segmentation instead of an instances segmentation. It means that each pixel will contain the answer to the question “is this pixel part of a microglia?” but the cells won’t be given individual IDs.</p></li>
</ul>
</section>
<section id="get-your-data-ready">
<h2>1. Get your data ready<a class="headerlink" href="#get-your-data-ready" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>You can retrain the model if you have some annotated data using the provided file: src/dl/unet2d_training.py</p></li>
<li><p>To train new UNet models, you need the file “src/dl/unet2d_training.py”. It contains the entire workflow to produce a bundled model ready for deployment.</p></li>
<li><p>Before starting, create a folder named “models” to store all the new model versions you create.</p></li>
<li><p>You also need a “working_dir” where the script will export its temporary data.</p></li>
<li><dl class="simple">
<dt>To train the UNet model, you need two distinct folders. You can name them as you like.</dt><dd><ul>
<li><p>The first folder, referred to as “inputs”, will contain “.tif” images with values globally normalized in the range [0.0, 1.0].</p></li>
<li><p>The second folder, referred to as “masks”, will also contain “.tif” images, but these will be binary masks. They are thresholded to everything above 0 upon opening, so there is no restriction on whether they should be 0 and 1 or 0 and 255.</p></li>
<li><p>Images in both folders should be named the same way.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The models produced by this script include:</dt><dd><ul>
<li><p>“version.txt”: The version index of this model, allowing detection if the model should be re-downloaded from the internet.</p></li>
<li><dl class="simple">
<dt>“training_history.png”: A set of 2 plots (with 4 plot slots).</dt><dd><ul>
<li><p>The first plot contains the loss and the validation loss.</p></li>
<li><p>The second plot contains the training and validation precision, as well as the training and validation recall.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>“last.keras”: The weights generated at the last training epoch.</p></li>
<li><p>“best.keras”: The weights that achieved the best validation loss. These weights are used in the segmentation step.</p></li>
<li><p>“data_usage.json”: Information on which files were used for training and which were used for validation.</p></li>
<li><p>“augmentations_preview.png”: A sample of data after passing through the data augmentation pipeline.</p></li>
<li><p>“architecture.png”: A graph representing the UNet2D architecture used by this model.</p></li>
<li><p>“predictions”: A folder containing the validation data and the model’s predictions.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="data-augmentation">
<h2>2. Data augmentation<a class="headerlink" href="#data-augmentation" title="Link to this heading"></a></h2>
<p>The images on which we have to work have a wide variance, so we need a solid data augmentation workflow to avoid having to annotate an unreasonable number of images, and to ensure that the model generalizes well to different types of data.</p>
<p>The data augmentation pipeline includes the following transformations:</p>
<ul class="simple">
<li><p><strong>Random rotations</strong>: The images are randomly rotated in a range from -90 to 90 degrees.</p></li>
<li><p><strong>Random flips</strong>: The images are randomly flipped horizontally and/or vertically.</p></li>
<li><p><strong>Random gamma adjustment</strong>: A random gamma correction is applied to every patch. It allows to spread or dilate the histogram.</p></li>
<li><p><strong>Random noise addition</strong>: Random Gaussian noise is added to the images.</p></li>
<li><p><strong>Filament ruptures</strong>: On many images, the filaments pass on another Z plane before coming back. To simulate that, some filamentous areas are randomly discarded (blurred).</p></li>
</ul>
<p>These augmentations are applied on-the-fly at loading to ensure that each epoch sees a different set of augmented images, which helps in improving the robustness and generalization of the model.</p>
</section>
<section id="filaments-extraction">
<h2>3. Filaments extraction<a class="headerlink" href="#filaments-extraction" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Images containing filaments actually contain 96% of background, which represents a massive imbalance between the background and foreground classes.</p></li>
<li><p>Using a usual loss function would end-up in the model predicting only solid black patches, as it would consider that it is 96% correct.</p></li>
<li><p>To address that problem, we had to re-implement the clDice loss as described in the paper: <a class="reference external" href="https://doi.org/10.48550/arXiv.2404.00130">https://doi.org/10.48550/arXiv.2404.00130</a>.</p></li>
</ul>
</section>
<section id="setup">
<h2>4. Setup<a class="headerlink" href="#setup" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>If you already have a Python environment in which “Microglia Analyzer” is installed, it already contains everything you need to train a model.</p></li>
<li><p>To launch the training, you just have to fill the settings in the first section, and run the script.</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>data_folder</p></td>
<td><p>Parent folder of the “inputs” and “masks” folders.</p></td>
</tr>
<tr class="row-odd"><td><p>qc_folder</p></td>
<td><p>Parent folder of the “inputs” and “masks” folders used only for quality control (not for training). These are just images used
to perform performance metrics at the end of training.</p></td>
</tr>
<tr class="row-even"><td><p>inputs_name</p></td>
<td><p>Name of the folder containing the input images (name of the folder in <cite>data_folder</cite> and <cite>qc_folder</cite>).</p></td>
</tr>
<tr class="row-odd"><td><p>masks_name</p></td>
<td><p>Name of the folder containing the masks (name of the folder in <cite>data_folder</cite> and <cite>qc_folder</cite>).</p></td>
</tr>
<tr class="row-even"><td><p>models_path</p></td>
<td><p>Folder in which the models will be saved. They will be saved as “{model_name_prefix}-V{version_number}”.</p></td>
</tr>
<tr class="row-odd"><td><p>working_directory</p></td>
<td><p>Folder in which the training, validation, and testing folders will be created. This folder and its content can be deleted once
the training is done.</p></td>
</tr>
<tr class="row-even"><td><p>model_name_prefix</p></td>
<td><p>Prefix of the model name. Will be part of the folder name in <cite>models_path</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p>reset_local_data</p></td>
<td><p>If True, the locally copied training, validation, and testing folders will be re-imported.</p></td>
</tr>
<tr class="row-even"><td><p>remove_wrong_data</p></td>
<td><p>If True, the data that is not useful will be deleted from the data folder. It is a destructive operation, review the first run
of the sanity check before activating this.</p></td>
</tr>
<tr class="row-odd"><td><p>data_usage</p></td>
<td><p>Path to a JSON file containing how each input file should be used (for training, validation, or testing).</p></td>
</tr>
<tr class="row-even"><td><p>validation_percentage</p></td>
<td><p>Percentage of the data that will be used for validation. This data will be moved to the validation folder.</p></td>
</tr>
<tr class="row-odd"><td><p>batch_size</p></td>
<td><p>Number of images per batch.</p></td>
</tr>
<tr class="row-even"><td><p>epochs</p></td>
<td><p>Number of epochs for the training.</p></td>
</tr>
<tr class="row-odd"><td><p>unet_depth</p></td>
<td><p>Depth of the UNet model, i.e., the number of layers in the encoder part (equal to the number of layers in the decoder part).</p></td>
</tr>
<tr class="row-even"><td><p>num_filters_start</p></td>
<td><p>Number of filters in the first layer of the UNet.</p></td>
</tr>
<tr class="row-odd"><td><p>dropout_rate</p></td>
<td><p>Dropout rate. Percentage of neurons that will be randomly disabled at each epoch. Better for generalization.</p></td>
</tr>
<tr class="row-even"><td><p>optimizer</p></td>
<td><p>Optimizer used for the training.</p></td>
</tr>
<tr class="row-odd"><td><p>learning_rate</p></td>
<td><p>Learning rate at which the optimizer is initialized.</p></td>
</tr>
<tr class="row-even"><td><p>skeleton_coef</p></td>
<td><p>Coefficient of the skeleton loss.</p></td>
</tr>
<tr class="row-odd"><td><p>bce_coef</p></td>
<td><p>Coefficient of the binary cross-entropy loss.</p></td>
</tr>
<tr class="row-even"><td><p>early_stop_patience</p></td>
<td><p>Number of epochs without improvement before stopping the training.</p></td>
</tr>
<tr class="row-odd"><td><p>dilation_kernel</p></td>
<td><p>Kernel used for the dilation of the skeleton.</p></td>
</tr>
<tr class="row-even"><td><p>loss</p></td>
<td><p>Loss function used for the training.</p></td>
</tr>
<tr class="row-odd"><td><p>use_data_augmentation</p></td>
<td><p>If True, data augmentation will be used.</p></td>
</tr>
<tr class="row-even"><td><p>use_mirroring</p></td>
<td><p>If True, random mirroring will be used.</p></td>
</tr>
<tr class="row-odd"><td><p>use_gaussian_noise</p></td>
<td><p>If True, random Gaussian noise will be used.</p></td>
</tr>
<tr class="row-even"><td><p>noise_scale</p></td>
<td><p>Scale of the Gaussian noise (range of values).</p></td>
</tr>
<tr class="row-odd"><td><p>use_random_rotations</p></td>
<td><p>If True, random rotations will be used.</p></td>
</tr>
<tr class="row-even"><td><p>angle_range</p></td>
<td><p>Range of the random rotations. The angle will be in [angle_range[0], angle_range[1]].</p></td>
</tr>
<tr class="row-odd"><td><p>use_gamma_correction</p></td>
<td><p>If True, random gamma correction will be used.</p></td>
</tr>
<tr class="row-even"><td><p>gamma_range</p></td>
<td><p>Range of the gamma correction. The gamma will be in [1 - gamma_range, 1 + gamma_range] (1.0 == neutral).</p></td>
</tr>
<tr class="row-odd"><td><p>use_holes</p></td>
<td><p>If True, holes will be created in the input images to teach the network to fill them.</p></td>
</tr>
<tr class="row-even"><td><p>export_aug_sample</p></td>
<td><p>If True, an augmented sample will be exported to the working directory as a preview.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="usage">
<h2>5. Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>This model consumes patches of 512×512 pixels, with an overlap of 128 pixels.</p></li>
<li><p>The merging is performed with the alpha-blending technique described on the page where the patches creation is explained.</p></li>
<li><p>The output is labeled by connected components and filtered by number of pixels (processed from a minimal area in µm²) before being presented to the user.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mga_user_guide.html" class="btn btn-neutral float-left" title="How to use the “Microglia Analyzer” widget?" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="classification.html" class="btn btn-neutral float-right" title="Classification using a YOLOv5" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Clément H. Benedetti.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>